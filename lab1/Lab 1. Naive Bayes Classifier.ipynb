{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment one: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Preprocessed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the preprocessed data:\n",
    "# <author> <book> <source_file> <text>\n",
    "# Create:\n",
    "# (<author>, <text>)\n",
    "def read_corpus(corpus_file):\n",
    "    out = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            out.append( (tokens[0], tokens[3:]) )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_CX = read_corpus(\"Data/Training_Set.txt\")\n",
    "test_CX = read_corpus(\"Data/Test_Set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Eliot\n",
      "x0 i\n",
      "x1 don\n",
      "x2 t\n",
      "x3 know\n",
      "x4 said\n",
      "x5 maggie\n"
     ]
    }
   ],
   "source": [
    "for C, X in test_CX:\n",
    "    print('category:', C)\n",
    "    for i,x in enumerate(X):\n",
    "        print('x%d' % i, x)\n",
    "    # jsut to avoid printing megabytes of text\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a classifier which decides best category for a given feature list based on likelihood of label for the features: $P(C_k\\ |\\ \\mathbf{x})$\n",
    "\n",
    "where, $\\mathbf{x} = (x_0, x_1, ..., x_{n-1})$ is the list of features (such as words) and $C_k$ is the category $k$. e.g. if there are only two categories (two possible authors) $\\{C_1, C_2\\}$.\n",
    "\n",
    "\n",
    "Which basically means that your classifier should be able to compute the liklihood of each category based on this bayesian model:\n",
    "\n",
    "\\begin{equation}\n",
    "P(C_k\\ |\\ \\mathbf{x}) = \\frac{P(\\mathbf{x}\\ |\\ C_k) P(C_k)}{P(\\mathbf{x})}\n",
    "\\end{equation}\n",
    "\n",
    "A classifier, in other word, will perform *argmax* on likihoods of categories with a given $\\mathbf{x}$:\n",
    "\n",
    "\\begin{equation}\n",
    "category\\_of(\\mathbf{x}) = \\mathbf{argmax}_{k \\in \\{1,2\\}}{P(C_k\\ |\\ \\mathbf{x})}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "A *naive* assumption is that all words are independent will relax the bayesian model based on following results from independence assumption:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{r c l}\n",
    "P(\\mathbf{x}) &=& P(x_0)\\ P(x_1)\\ ...\\ P(x_{n-1}) \\\\\n",
    "P(\\mathbf{x}\\ |\\ C_k) &=& P(x_0\\ |\\ C_k)\\ P(x_1\\ |\\ C_k)\\ ... P(x_{n-1}\\ |\\ C_k)\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "You can use the frequentist interpretation to compute the probabilities in the model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{r c l}\n",
    "P(x_i) &=& \\frac{freq(x_i)}{freq(any\\_word)} \\\\\n",
    "P(C_k) &=& \\frac{freq(C_k)}{freq(any\\_category)} \\\\\n",
    "P(x_i\\ |\\ C_k) &=& \\frac{freq(x_i,\\ C_k)}{freq(any\\_word,\\ C_k)} = \\frac{P(x_i,\\ C_k)}{P(C_k)} \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "In python, you should store these as dictionaries in and we call it the naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_train(training_data):\n",
    "    \"\"\"\n",
    "    training_data: a list of tuples (category, X=(list of words))\n",
    "    \"\"\"\n",
    "    # TODO: create all components of the model\n",
    "    return # (tuple of dictionaries)\n",
    "\n",
    "def nb_classifier(X, model):\n",
    "    \"\"\"\n",
    "    X: a list of words\n",
    "    model: a tuple of dictionaries\n",
    "    \"\"\"\n",
    "    # TODO: implement the bayesian model based on naive assumption of independence between words\n",
    "    \n",
    "    return # the most likely category\n",
    "    \n",
    "    \n",
    "model = nb_train(train_CX)\n",
    "\n",
    "# True and False for each sample in test\n",
    "results = [nb_classifier(X, model) == C for C, X in test_CX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hints on using dictionary in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to create a frequency dictionry in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a sample bag of words from training set for purpose of this programing tip\n",
    "eliot_sentence = ['anybody', 'might', 'think', 'now', 'you', 'had', 'a', 'right', 'to', 'give', 'yourself', 'a', 'little', 'comfort']\n",
    "articles = ['a', 'an', 'the']\n",
    "\n",
    "# Goals: \n",
    "# 1. Create a ditionry of words and their frequency.\n",
    "# 2. Use this dicitonry to say how many of each word in given articles occured in this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'a': 2, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1}\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = dict()\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    if word in eliot_words:\n",
    "        eliot_words[word] += 1\n",
    "    else:\n",
    "        eliot_words[word] = 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    if word in eliot_words:\n",
    "        print(word, eliot_words[word])\n",
    "    else:\n",
    "        print(word, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collections.Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 2, 'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1})\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = Counter()\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    eliot_words[word] += 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    print(word, eliot_words[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collections.defaultdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'a': 2, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1})\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = defaultdict(int)\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    eliot_words[word] += 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    print(word, eliot_words[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
