{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment one: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Preprocessed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the preprocessed data:\n",
    "# <author> <book> <source_file> <text>\n",
    "# Create:\n",
    "# (<author>, <text>)\n",
    "def read_corpus(corpus_file):\n",
    "    out = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            out.append( (tokens[0], tokens[3:]) )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_CX = read_corpus(\"Data/Training_Set.txt\")\n",
    "test_CX = read_corpus(\"Data/Test_Set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is just for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Eliot\n",
      "x0 i\n",
      "x1 don\n",
      "x2 t\n",
      "x3 know\n",
      "x4 said\n",
      "x5 maggie\n"
     ]
    }
   ],
   "source": [
    "for C, X in test_CX:\n",
    "    print('category:', C)\n",
    "    for i,x in enumerate(X):\n",
    "        print('x%d' % i, x)\n",
    "    # break just to avoid printing megabytes of text\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a classifier which decides best category for a given feature list based on likelihood of label for the features: $P(C_k\\ |\\ \\mathbf{x})$\n",
    "\n",
    "where, $\\mathbf{x} = (x_0, x_1, ..., x_{n-1})$ is the list of features (such as words) and $C_k$ is the category $k$. e.g. if there are only two categories (two possible authors) $\\{C_1, C_2\\}$.\n",
    "\n",
    "\n",
    "Which basically means that your classifier should be able to compute the liklihood of each category based on this bayesian model:\n",
    "\n",
    "\\begin{equation}\n",
    "P(C_k\\ |\\ \\mathbf{x}) = \\frac{P(\\mathbf{x}\\ |\\ C_k) P(C_k)}{P(\\mathbf{x})}\n",
    "\\end{equation}\n",
    "\n",
    "A classifier, in other word, will perform *argmax* on likihoods of categories with a given $\\mathbf{x}$:\n",
    "\n",
    "\\begin{equation}\n",
    "category\\_of(\\mathbf{x}) = \\mathbf{argmax}_{k \\in \\{1,2\\}}{P(C_k\\ |\\ \\mathbf{x})}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "A *naive* assumption is that all words are independent will relax the bayesian model based on following results from independence assumption:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{r c l}\n",
    "P(\\mathbf{x}) &=& P(x_0)\\ P(x_1)\\ ...\\ P(x_{n-1}) \\\\\n",
    "P(\\mathbf{x}\\ |\\ C_k) &=& P(x_0\\ |\\ C_k)\\ P(x_1\\ |\\ C_k)\\ ... P(x_{n-1}\\ |\\ C_k)\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "*Notice that our classification goal, $P(C_k\\ |\\ \\mathbf{x})$, cannot use chain rule of probabilities similar to $P(\\mathbf{x}\\ |\\ C_k)$.* \n",
    "\n",
    "\n",
    "You can use the frequentist interpretation to compute the probabilities in the model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{r c l}\n",
    "P(x_i) &=& \\frac{freq(x_i)}{freq(any\\_word)} \\\\\n",
    "P(C_k) &=& \\frac{freq(C_k)}{freq(any\\_category)} \\\\\n",
    "P(x_i\\ |\\ C_k) &=& \\frac{freq(x_i,\\ C_k)}{freq(any\\_word,\\ C_k)} = \\frac{P(x_i,\\ C_k)}{P(C_k)} \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "In python, you should store these as dictionaries in and we call it the naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_train(training_data):\n",
    "    \"\"\"\n",
    "    training_data: a list of tuples (category, X=(list of words))\n",
    "    \"\"\"\n",
    "    # TODO: create all components of the model\n",
    "    return # (tuple of dictionaries)\n",
    "\n",
    "def nb_classifier(X, model):\n",
    "    \"\"\"\n",
    "    X: a list of words\n",
    "    model: a tuple of dictionaries\n",
    "    \"\"\"\n",
    "    # TODO: implement the bayesian model based on naive assumption of independence between words\n",
    "    \n",
    "    return # the most likely category\n",
    "    \n",
    "    \n",
    "model = nb_train(train_CX)\n",
    "\n",
    "# True and False for each sample in test\n",
    "results = [nb_classifier(X, model) == C for C, X in test_CX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hints on using dictionary in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to create a frequency dictionry in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a sample bag of words from training set for purpose of this programing tip\n",
    "eliot_sentence = ['anybody', 'might', 'think', 'now', 'you', 'had', 'a', 'right', 'to', 'give', 'yourself', 'a', 'little', 'comfort']\n",
    "articles = ['a', 'an', 'the']\n",
    "\n",
    "# Goals: \n",
    "# 1. Create a ditionry of words and their frequency.\n",
    "# 2. Use this dicitonry to say how many of each word in given articles occured in this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'a': 2, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1}\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = dict()\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    if word in eliot_words:\n",
    "        eliot_words[word] += 1\n",
    "    else:\n",
    "        eliot_words[word] = 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    if word in eliot_words:\n",
    "        print(word, eliot_words[word])\n",
    "    else:\n",
    "        print(word, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collections.Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 2, 'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1})\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = Counter()\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    eliot_words[word] += 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    print(word, eliot_words[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collections.defaultdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'anybody': 1, 'might': 1, 'think': 1, 'now': 1, 'you': 1, 'had': 1, 'a': 2, 'right': 1, 'to': 1, 'give': 1, 'yourself': 1, 'little': 1, 'comfort': 1})\n",
      "a 2\n",
      "an 0\n",
      "the 0\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "eliot_words = defaultdict(int)\n",
    "\n",
    "# count words:\n",
    "for word in eliot_sentence:\n",
    "    eliot_words[word] += 1\n",
    "\n",
    "print(eliot_words)\n",
    "                \n",
    "# report how many \n",
    "for word in articles:\n",
    "    print(word, eliot_words[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints on computational statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Division by zero is a common problem when you want to calculate probability of unknown symbol. In classification task, you can basically use [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid such problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without smoothing\n",
      "P(X=a) = 0.3333333333333333\n",
      "P(X=b) = 0.6666666666666666\n",
      "P(X=unk) = 0.0\n",
      "with smoothing\n",
      "P(X=a) = 0.3333333333333333\n",
      "P(X=b) = 0.6666633333666664\n",
      "P(X=unk) = 3.3333000003333303e-06\n"
     ]
    }
   ],
   "source": [
    "# frequentist probability with laplace smoothing\n",
    "\n",
    "# observations ('a' and 'b')\n",
    "freq_a   = 10\n",
    "freq_b   = 20\n",
    "\n",
    "print(\"without smoothing\")\n",
    "# 'unk' is an unobserved event, its probability is zero\n",
    "freq_unk = 0\n",
    "freq_any = freq_a + freq_b\n",
    "\n",
    "# P(X=a) = p_a, ...\n",
    "p_a   = freq_a / freq_any\n",
    "p_b   = freq_b / freq_any\n",
    "p_unk = freq_unk / freq_any\n",
    "\n",
    "print(\"P(X=a) =\", p_a)\n",
    "print(\"P(X=b) =\", p_b)\n",
    "print(\"P(X=unk) =\", p_unk)\n",
    "\n",
    "\n",
    "print(\"with smoothing\")\n",
    "# with laplace smoothing,\n",
    "# we consider a chance for unseen event,\n",
    "# to keep the probabilities relatively right, it adds same number of assumed observations to all events\n",
    "alpha = 0.0001\n",
    "freq_a   += alpha\n",
    "freq_b   += alpha\n",
    "freq_unk += alpha\n",
    "freq_any  = freq_a + freq_b + freq_unk\n",
    "\n",
    "p_a   = freq_a   / freq_any\n",
    "p_b   = freq_b   / freq_any\n",
    "p_unk = freq_unk / freq_any\n",
    "\n",
    "print(\"P(X=a) =\", p_a)\n",
    "print(\"P(X=b) =\", p_b)\n",
    "print(\"P(X=unk) =\", p_unk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the common computational problems in using small numbers is [arithmetic underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). A solution to avoid this problem is to use logarithmic numbers. Then, all multiplications and divisions should be translated to additions and subtractions between logarithmic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1e-200\n",
      "n = 1e+200\n",
      "a / n * n / a = 0.0\n",
      "a = 9.99999999999978e-201\n",
      "n = 1.000000000000022e+200\n",
      "a / n * n / a = 1.0\n"
     ]
    }
   ],
   "source": [
    "# very small and very big numbers\n",
    "a = 1.e-200\n",
    "n = 1.e200\n",
    "print('a =', a)\n",
    "print('n =', n)\n",
    "print('a / n * n / a =', a / n * n / a)\n",
    "\n",
    "# using log\n",
    "from math import log, exp\n",
    "log_a = log(a)\n",
    "log_n = log(n)\n",
    "\n",
    "print('a =',exp(log_a))\n",
    "print('n =',exp(log_n))\n",
    "print('a / n * n / a =', exp(log_a - log_n + log_n - log_a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequently asked questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are we allowed to use external packages sucha as `nltk.probability` in our implementation?\n",
    "    - Yes, only with one condition: The way that you use any package should demonstrate your understanding of 1. conditional probabilities and 2. naive bayes classifier. For example, if you use `FreqDist`, it's completly fine. If you use `ConditionalProbDist`, we may ask you to explain how it works and why it works in a short note. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
